# Postgres Multi-Headed Monster

## Description

This project aims to implement a shared disk architecture database, with an eye
on high-availability and horizontal scaling.

## Architecture Components

1. Cluster awareness and membership
2. Global transaction-ID generation
3. WAL generation
4. WAL consolidation
5. Block ownership by nodes
6. Waiting for/locking on remote transactions
7. Commit ordering
8. Node recovery
9. Transaction snapshot

## Cluster Awareness and Membership

The database nodes should be aware of other nodes in the cluster. The nodes
should agree on a protocol during cluster startup and shutdown, to avoid
stomping on each others' feet (for e.g. control-file data).

The nodes should also promptly be notified if a node leaves cluster membeship
for any reason. This prompt notification will allow the remaining nodes to take
corrective action for resources currently held by the exiting node, as well as
perform crash-reccovery for the node that left, if necessary.

## Global Transaction-ID Generation

The transaction ID assigned to each transaction across the cluster needs to be
unique, and needs to be assigned exactly in the order that the transaction
starts. Hence all nodes should synchronize the XID generation.

Leslie Lamport's clock seems to be an ideal algorithm to base the initial
implementaiton of this distributed synchronization on.

## WAL Generation

Since this is a shared-disk architecture, the data-directory structure as seen
by each node will be identical, hence the database nodes cannot all write their
WAL data to $PGDATA/pg_xlog/ direcotry. Each node will need to write WAL to its
own dedicated directory.

## WAL Consolidation

Since the WAL is now split across more than one set of files, there needs to
exist a mechanism that can reconstruct the WAL data from these multiple sources
and produce a result exactly as if generated by a single-node, non-clustered
database node.

This will be needed for cluter level crash recovery, straming replication and
archive recovery.

## Block Ownership By Nodes

Whenever a node wants to modify data on a block, it should take a
cluster-exclusive lock on the block before proceeding. This ensures that no two
database nodes write data in a block that may later be lost due to lack of
synchrnonization.

This also requires that when acuiring an exclusive lock on the block, the lock
acquirer fetches the latest block image from another nodei, if any, which last
modified the block.

Again, Leslie Lamport's Clock can be used to request exclusive lock access to
the blocks.

## Waiting For/Locking On Remote Transactions

Since the rows may be updated/deleted by transactions on remote nodes, the row
that a node is trying to update may be locked, and this node will have to wait
for the original transaction to either commit or abort before proceeding.

This behaviour is the same as in a single-node setup. The tricky part is in
tracing the commit/rollback of remote transactions.

For now, this will be achieved by every commit on a node to be communicated to
remote nodes, and those nodes signaling waiting transactions, if any. This may
later be optimized to notify only those nodes that are interested in transaction
being committed/rolled back.

## Commit Ordering

As long as the transactions don't interact with each other, that is, they don't
depend on commit/rollback of each other, they can be committed in any order. But
if a transaction B ever waited on transaction A, then transaction B cannot be
committed before A.

In MUHEAM we will not have to anything special for transactions that depend on
transactions from other nodes, since a local transaction will never proceed
until it has received a confirmation of the remote transaction's commit.

## Node Recovery

## Transaction Snapshot

???

## Dedication

This project is dedicated to my mother, Hrabhajan Kaur.
